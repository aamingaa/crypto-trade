{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8d499e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z7/t_44yjd97lvcw0phmtfrd79r0000gn/T/ipykernel_48520/1568939837.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from util.dollarbar_fusion import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mbuild_dollar_bars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0maggregate_trade_features_on_bars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from util.dollarbar_fusion import (\n",
    "    build_dollar_bars,\n",
    "    aggregate_trade_features_on_bars,\n",
    ")\n",
    " \n",
    "\n",
    "\n",
    "def _add_bar_lags_and_rollings(\n",
    "    Xb: pd.DataFrame,\n",
    "    add_lags: int = 2,\n",
    "    rolling_windows: Optional[List[int]] = None,\n",
    "    rolling_stats: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    X = Xb.copy().sort_index()\n",
    "\n",
    "    feature_cols = list(X.columns)\n",
    "\n",
    "    # lags\n",
    "    for k in range(1, add_lags + 1):\n",
    "        for col in feature_cols:\n",
    "            X[f'{col}_lag{k}'] = X[col].shift(k)\n",
    "\n",
    "    # rollings\n",
    "    if rolling_windows:\n",
    "        stats = rolling_stats or ['mean', 'std', 'sum']\n",
    "        for w in rolling_windows:\n",
    "            roll = X[feature_cols].rolling(window=w, min_periods=w)\n",
    "            for stat in stats:\n",
    "                if stat == 'mean':\n",
    "                    tmp = roll.mean()\n",
    "                elif stat == 'std':\n",
    "                    tmp = roll.std()\n",
    "                elif stat == 'sum':\n",
    "                    tmp = roll.sum()\n",
    "                elif stat == 'min':\n",
    "                    tmp = roll.min()\n",
    "                elif stat == 'max':\n",
    "                    tmp = roll.max()\n",
    "                else:\n",
    "                    continue\n",
    "                tmp.columns = [f'{col}_roll{w}_{stat}' for col in X[feature_cols].columns]\n",
    "                X = X.join(tmp)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def _time_splits_purged(\n",
    "    idx: pd.DatetimeIndex,\n",
    "    n_splits: int = 5,\n",
    "    embargo: str = '0H',\n",
    ") -> List[Tuple[pd.DatetimeIndex, pd.DatetimeIndex]]:\n",
    "    \"\"\"\n",
    "    生成时间连续的折，返回 (train_index, test_index) 对列表。\n",
    "    训练集对测试集边界施加 Embargo，避免信息泄露。\n",
    "    \"\"\"\n",
    "    times = pd.Series(index=idx.unique().sort_values(), data=np.arange(len(idx.unique())))\n",
    "    n = len(times)\n",
    "    if n_splits < 2 or n < n_splits:\n",
    "        raise ValueError('样本过少，无法进行时间序列CV')\n",
    "\n",
    "    fold_sizes = [n // n_splits] * n_splits\n",
    "    for i in range(n % n_splits):\n",
    "        fold_sizes[i] += 1\n",
    "\n",
    "    # 计算各折在时间索引上的切片范围\n",
    "    boundaries = []\n",
    "    start = 0\n",
    "    for sz in fold_sizes:\n",
    "        end = start + sz\n",
    "        boundaries.append((start, end))\n",
    "        start = end\n",
    "\n",
    "    embargo_td = pd.Timedelta(embargo)\n",
    "    out: List[Tuple[pd.DatetimeIndex, pd.DatetimeIndex]] = []\n",
    "    for (s, e) in boundaries:\n",
    "        test_times = times.index[s:e]\n",
    "        test_mask = idx.isin(test_times)\n",
    "\n",
    "        test_start = test_times.min()\n",
    "        test_end = test_times.max()\n",
    "\n",
    "        left_block = (idx <= (test_start + embargo_td))\n",
    "        right_block = (idx >= (test_end - embargo_td))\n",
    "        exclude = left_block | right_block | test_mask\n",
    "        train_idx = idx[~exclude]\n",
    "        test_idx = idx[test_mask]\n",
    "        if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "            continue\n",
    "        out.append((train_idx, test_idx))\n",
    "    return out\n",
    "\n",
    "\n",
    "def purged_cv_evaluate(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_splits: int = 5,\n",
    "    embargo: str = '1H',\n",
    "    model_type: str = 'ridge',\n",
    "    random_state: int = 42,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    使用 Purged 时间序列 CV 进行回归评估，返回按折与汇总的指标。\n",
    "    指标：Pearson IC、Spearman IC、RMSE、方向准确率。\n",
    "    \"\"\"\n",
    "    assert X.index.equals(y.index)\n",
    "\n",
    "    # 选择模型\n",
    "    if model_type == 'rf':\n",
    "        base_model = RandomForestRegressor(\n",
    "            n_estimators=300, max_depth=8, random_state=random_state, n_jobs=-1\n",
    "        )\n",
    "        use_scaler = False\n",
    "    else:\n",
    "        base_model = Ridge(alpha=1.0, random_state=random_state)\n",
    "        use_scaler = True\n",
    "\n",
    "    splits = _time_splits_purged(X.index, n_splits=n_splits, embargo=embargo)\n",
    "    by_fold = []\n",
    "    preds_all = pd.Series(index=X.index, dtype=float)\n",
    "\n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(splits):\n",
    "        Xtr, ytr = X.loc[tr_idx], y.loc[tr_idx]\n",
    "        Xte, yte = X.loc[te_idx], y.loc[te_idx]\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler = StandardScaler()\n",
    "            Xtr_scaled = pd.DataFrame(\n",
    "                scaler.fit_transform(Xtr.values), index=Xtr.index, columns=Xtr.columns\n",
    "            )\n",
    "            Xte_scaled = pd.DataFrame(\n",
    "                scaler.transform(Xte.values), index=Xte.index, columns=Xte.columns\n",
    "            )\n",
    "        else:\n",
    "            Xtr_scaled, Xte_scaled = Xtr, Xte\n",
    "\n",
    "        model = base_model\n",
    "        model.fit(Xtr_scaled, ytr)\n",
    "        yhat = pd.Series(model.predict(Xte_scaled), index=te_idx)\n",
    "        preds_all.loc[te_idx] = yhat\n",
    "\n",
    "        # 指标\n",
    "        pearson_ic = yhat.corr(yte)\n",
    "        spearman_ic = yhat.corr(yte, method='spearman')\n",
    "        rmse = mean_squared_error(yte, yhat) ** 0.5\n",
    "        dir_acc = (np.sign(yhat) == np.sign(yte)).mean()\n",
    "        by_fold.append({\n",
    "            'fold': fold_id,\n",
    "            'pearson_ic': float(pearson_ic),\n",
    "            'spearman_ic': float(spearman_ic),\n",
    "            'rmse': float(rmse),\n",
    "            'dir_acc': float(dir_acc),\n",
    "            'n_train': int(len(Xtr)),\n",
    "            'n_test': int(len(Xte)),\n",
    "        })\n",
    "\n",
    "    # 汇总\n",
    "    df_folds = pd.DataFrame(by_fold)\n",
    "    summary = {\n",
    "        'pearson_ic_mean': float(df_folds['pearson_ic'].mean()) if not df_folds.empty else np.nan,\n",
    "        'spearman_ic_mean': float(df_folds['spearman_ic'].mean()) if not df_folds.empty else np.nan,\n",
    "        'rmse_mean': float(df_folds['rmse'].mean()) if not df_folds.empty else np.nan,\n",
    "        'dir_acc_mean': float(df_folds['dir_acc'].mean()) if not df_folds.empty else np.nan,\n",
    "        'n_splits_effective': int(len(df_folds)),\n",
    "    }\n",
    "    return {\n",
    "        'by_fold': by_fold,\n",
    "        'summary': summary,\n",
    "        'predictions': preds_all,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_barlevel_dataset(\n",
    "    trades: pd.DataFrame,\n",
    "    dollar_threshold: float,\n",
    "    horizon_bars: int = 1,\n",
    "    add_lags: int = 2,\n",
    "    rolling_windows: Optional[List[int]] = None,\n",
    "    rolling_stats: Optional[List[str]] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    逐笔 -> Dollar Bars -> Bar特征 -> N-bar 标签（不做小时对齐）。\n",
    "    返回：X_bar, y_bar, bars, bar_features\n",
    "    \"\"\"\n",
    "    # 1) 事件Bar\n",
    "    bars = build_dollar_bars(trades, dollar_threshold=dollar_threshold)\n",
    "    if bars.empty:\n",
    "        return pd.DataFrame(), pd.Series(dtype=float), bars, pd.DataFrame()\n",
    "\n",
    "    # 2) Bar级特征（逐笔交易侧）\n",
    "    bar_feat = aggregate_trade_features_on_bars(trades, bars)\n",
    "\n",
    "    # 3) 标签：未来 N 根 bar 的对数收益\n",
    "    close = bar_feat['close'] if 'close' in bar_feat.columns else bars.set_index('bar_id')['close']\n",
    "    y_bar = np.log(close.shift(-horizon_bars) / close)\n",
    "\n",
    "    # 4) 特征工程：去掉时间列，仅保留数值特征\n",
    "    keep_cols = [c for c in bar_feat.columns if c not in ['start_time', 'end_time']]\n",
    "    X_bar = bar_feat[keep_cols]\n",
    "    X_bar = _add_bar_lags_and_rollings(\n",
    "        X_bar,\n",
    "        add_lags=add_lags,\n",
    "        rolling_windows=rolling_windows,\n",
    "        rolling_stats=rolling_stats,\n",
    "    )\n",
    "\n",
    "    # 5) 对齐与去NaN\n",
    "    X_bar = X_bar.dropna()\n",
    "    y_bar = y_bar.loc[X_bar.index]\n",
    "\n",
    "    return X_bar, y_bar, bars, bar_feat\n",
    "\n",
    "\n",
    "def run_barlevel_pipeline(\n",
    "    trades: pd.DataFrame,\n",
    "    dollar_threshold: float,\n",
    "    horizon_bars: int = 1,\n",
    "    add_lags: int = 2,\n",
    "    rolling_windows: Optional[List[int]] = None,\n",
    "    rolling_stats: Optional[List[str]] = None,\n",
    "    n_splits: int = 5,\n",
    "    embargo_bars: int = 0,\n",
    "    model_type: str = 'ridge',\n",
    "    random_state: int = 42,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    端到端：逐笔 -> 事件Bar -> Bar级特征 -> Purged CV（按bar索引，使用等距切片近似）\n",
    "    注意：这里的 purged 分割在 bar 索引上做，embargo_bars 控制左右留白根数。\n",
    "    \"\"\"\n",
    "    Xb, yb, bars, bar_feat = make_barlevel_dataset(\n",
    "        trades=trades,\n",
    "        dollar_threshold=dollar_threshold,\n",
    "        horizon_bars=horizon_bars,\n",
    "        add_lags=add_lags,\n",
    "        rolling_windows=rolling_windows,\n",
    "        rolling_stats=rolling_stats,\n",
    "    )\n",
    "\n",
    "    if Xb.empty or yb.empty:\n",
    "        return {\n",
    "            'error': '数据不足或阈值设置过大，无法构造训练集（bar级）',\n",
    "            'X_bar': Xb,\n",
    "            'y_bar': yb,\n",
    "            'bars': bars,\n",
    "            'bar_features': bar_feat,\n",
    "        }\n",
    "\n",
    "    # 使用真实 bar 的 end_time 作为时间索引\n",
    "    bar_times = bars.set_index('bar_id')['end_time']\n",
    "    idx_time = pd.to_datetime(bar_times.loc[Xb.index])\n",
    "    Xb2 = Xb.copy(); Xb2.index = idx_time\n",
    "    yb2 = yb.copy(); yb2.index = idx_time\n",
    "\n",
    "    # 将 embargo_bars 转换为时间间隔：使用 bar 的中位时长作为单位\n",
    "    bar_durations = (bars['end_time'] - bars['start_time']).dropna()\n",
    "    median_duration = bar_durations.median() if not bar_durations.empty else pd.Timedelta(0)\n",
    "    embargo = median_duration * int(max(0, embargo_bars))\n",
    "    eval_result = purged_cv_evaluate(\n",
    "        X=Xb2,\n",
    "        y=yb2,\n",
    "        n_splits=n_splits,\n",
    "        embargo=embargo,\n",
    "        model_type=model_type,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    out = {\n",
    "        'eval': eval_result,\n",
    "        'X_bar': Xb,\n",
    "        'y_bar': yb,\n",
    "        'bars': bars,\n",
    "        'bar_features': bar_feat,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def _factor_int_trade_stats(seg: pd.DataFrame, start_ts: pd.Timestamp, end_ts: pd.Timestamp) -> Dict[str, float]:\n",
    "    if seg.empty:\n",
    "        duration_seconds = max(1.0, (end_ts - start_ts).total_seconds())\n",
    "        return {\n",
    "            'int_trade_vwap': np.nan,\n",
    "            'int_trade_volume_sum': 0.0,\n",
    "            'int_trade_dollar_sum': 0.0,\n",
    "            'int_trade_signed_volume': 0.0,\n",
    "            'int_trade_buy_ratio': np.nan,\n",
    "            'int_trade_intensity': 0.0,\n",
    "            'int_trade_rv': np.nan,\n",
    "        }\n",
    "    dollar = seg['quote_qty'].sum()\n",
    "    qty_sum = seg['qty'].sum()\n",
    "    vwap = (seg['price'] * seg['qty']).sum() / qty_sum if qty_sum > 0 else np.nan\n",
    "    signed_volume = (seg['qty'] * seg['trade_sign']).sum()\n",
    "    buy_ratio = seg.loc[seg['trade_sign'] > 0, 'qty'].sum() / qty_sum if qty_sum > 0 else np.nan\n",
    "    seg = seg.copy()\n",
    "    seg['logp'] = np.log(seg['price'])\n",
    "    rv = (seg['logp'].diff().dropna() ** 2).sum()\n",
    "    duration_seconds = max(1.0, (end_ts - start_ts).total_seconds())\n",
    "    intensity = len(seg) / duration_seconds\n",
    "    return {\n",
    "        'int_trade_vwap': float(vwap) if pd.notna(vwap) else np.nan,\n",
    "        'int_trade_volume_sum': float(qty_sum),\n",
    "        'int_trade_dollar_sum': float(dollar),\n",
    "        'int_trade_signed_volume': float(signed_volume),\n",
    "        'int_trade_buy_ratio': float(buy_ratio) if pd.notna(buy_ratio) else np.nan,\n",
    "        'int_trade_intensity': float(intensity),\n",
    "        'int_trade_rv': float(rv) if pd.notna(rv) else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "def _factor_order_flow_imbalance(seg: pd.DataFrame) -> Dict[str, float]:\n",
    "    if seg.empty:\n",
    "        return {\n",
    "            'ofi_signed_qty_sum': 0.0,\n",
    "            'ofi_signed_quote_sum': 0.0,\n",
    "            'ofi_trade_count_diff': 0.0,\n",
    "        }\n",
    "    signed_qty = float((seg['qty'] * seg['trade_sign']).sum())\n",
    "    signed_quote = float((seg['quote_qty'] * seg['trade_sign']).sum())\n",
    "    trade_count_diff = float((seg['trade_sign'] > 0).sum() - (seg['trade_sign'] < 0).sum())\n",
    "    return {\n",
    "        'ofi_signed_qty_sum': signed_qty,\n",
    "        'ofi_signed_quote_sum': signed_quote,\n",
    "        'ofi_trade_count_diff': trade_count_diff,\n",
    "    }\n",
    "\n",
    "\n",
    "def _factor_garman_order_flow(seg: pd.DataFrame) -> Dict[str, float]:\n",
    "    if seg.empty:\n",
    "        return {'gof_by_count': np.nan, 'gof_by_volume': np.nan}\n",
    "    n = len(seg)\n",
    "    qty_sum = seg['qty'].sum()\n",
    "    gof_by_count = float(seg['trade_sign'].sum()) / n if n > 0 else np.nan\n",
    "    gof_by_volume = float((seg['trade_sign'] * seg['qty']).sum()) / qty_sum if qty_sum > 0 else np.nan\n",
    "    return {\n",
    "        'gof_by_count': gof_by_count,\n",
    "        'gof_by_volume': gof_by_volume,\n",
    "    }\n",
    "\n",
    "\n",
    "def _factor_rolling_ofi(seg: pd.DataFrame) -> Dict[str, float]:\n",
    "    if seg.empty:\n",
    "        return {'ofi_roll_sum_max': 0.0, 'ofi_roll_sum_std': 0.0}\n",
    "    n = len(seg)\n",
    "    roll_n = max(1, min(20, n))\n",
    "    signed_qty_series = (seg['trade_sign'] * seg['qty']).astype(float).reset_index(drop=True)\n",
    "    ofi_roll = signed_qty_series.rolling(window=roll_n, min_periods=roll_n).sum()\n",
    "    ofi_roll_max = float(ofi_roll.max()) if len(ofi_roll.dropna()) else 0.0\n",
    "    ofi_roll_std = float(ofi_roll.std()) if len(ofi_roll.dropna()) else 0.0\n",
    "    return {'ofi_roll_sum_max': ofi_roll_max, 'ofi_roll_sum_std': ofi_roll_std}\n",
    "\n",
    "\n",
    "def _factor_activity_and_persistence(seg: pd.DataFrame) -> Dict[str, float]:\n",
    "    if seg.empty:\n",
    "        return {\n",
    "            'runlen_buy_max': 0.0,\n",
    "            'runlen_sell_max': 0.0,\n",
    "            'runlen_buy_mean': np.nan,\n",
    "            'runlen_sell_mean': np.nan,\n",
    "            'alt_frequency': np.nan,\n",
    "            'p_pos_to_pos': np.nan,\n",
    "            'p_neg_to_neg': np.nan,\n",
    "        }\n",
    "    sgn = seg['trade_sign'].astype(int).to_numpy()\n",
    "    n = len(sgn)\n",
    "    # run-lengths\n",
    "    run_lengths = []\n",
    "    cur = sgn[0]\n",
    "    length = 1\n",
    "    for val in sgn[1:]:\n",
    "        if val == cur:\n",
    "            length += 1\n",
    "        else:\n",
    "            run_lengths.append((cur, length))\n",
    "            cur = val\n",
    "            length = 1\n",
    "    run_lengths.append((cur, length))\n",
    "    buy_runs = [l for s, l in run_lengths if s > 0]\n",
    "    sell_runs = [l for s, l in run_lengths if s < 0]\n",
    "    runlen_buy_max = float(max(buy_runs)) if buy_runs else 0.0\n",
    "    runlen_sell_max = float(max(sell_runs)) if sell_runs else 0.0\n",
    "    runlen_buy_mean = float(np.mean(buy_runs)) if buy_runs else np.nan\n",
    "    runlen_sell_mean = float(np.mean(sell_runs)) if sell_runs else np.nan\n",
    "    # 交替频率\n",
    "    flips = int((np.diff(sgn) != 0).sum()) if n > 1 else 0\n",
    "    alt_frequency = flips / (n - 1) if n > 1 else np.nan\n",
    "    # Markov 转移概率\n",
    "    p_pos_to_pos = np.nan\n",
    "    p_neg_to_neg = np.nan\n",
    "    if n > 1:\n",
    "        from_pos = sgn[:-1] > 0\n",
    "        to_pos = sgn[1:] > 0\n",
    "        from_neg = sgn[:-1] < 0\n",
    "        to_neg = sgn[1:] < 0\n",
    "        pos_count = int(from_pos.sum())\n",
    "        neg_count = int(from_neg.sum())\n",
    "        p_pos_to_pos = float((from_pos & to_pos).sum() / pos_count) if pos_count > 0 else np.nan\n",
    "        p_neg_to_neg = float((from_neg & to_neg).sum() / neg_count) if neg_count > 0 else np.nan\n",
    "    return {\n",
    "        'runlen_buy_max': runlen_buy_max,\n",
    "        'runlen_sell_max': runlen_sell_max,\n",
    "        'runlen_buy_mean': runlen_buy_mean,\n",
    "        'runlen_sell_mean': runlen_sell_mean,\n",
    "        'alt_frequency': float(alt_frequency) if pd.notna(alt_frequency) else np.nan,\n",
    "        'p_pos_to_pos': p_pos_to_pos,\n",
    "        'p_neg_to_neg': p_neg_to_neg,\n",
    "    }\n",
    "\n",
    "\n",
    "def _compute_interval_trade_features(trades: pd.DataFrame, start_ts: pd.Timestamp, end_ts: pd.Timestamp) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    在给定时间区间 [start_ts, end_ts) 内，逐笔计算区间特征。\n",
    "    每个特征由独立函数实现并组合返回。\n",
    "    \"\"\"\n",
    "    df = trades.copy()\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "    df = df.sort_values('time')\n",
    "    if 'quote_qty' not in df.columns or df['quote_qty'].isna().all():\n",
    "        df['quote_qty'] = df['price'] * df['qty']\n",
    "    df['trade_sign'] = np.where(df['is_buyer_maker'], -1, 1)\n",
    "    seg = df.loc[(df['time'] >= start_ts) & (df['time'] < end_ts)].copy()\n",
    "\n",
    "    out = {}\n",
    "    out.update(_factor_int_trade_stats(seg, start_ts, end_ts))\n",
    "    out.update(_factor_order_flow_imbalance(seg))\n",
    "    out.update(_factor_garman_order_flow(seg))\n",
    "    out.update(_factor_rolling_ofi(seg))\n",
    "    out.update(_factor_activity_and_persistence(seg))\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_interval_feature_dataset(\n",
    "    trades: pd.DataFrame,\n",
    "    dollar_threshold: float,\n",
    "    feature_window_bars: int = 10,\n",
    "    horizon_bars: int = 3,\n",
    "    window_mode: str = 'past',  # 'past' 使用过去N个bar的[start,end)，'future' 使用未来N个bar（注意可能泄露）\n",
    "    add_lags: int = 0,\n",
    "    rolling_windows: Optional[List[int]] = None,\n",
    "    rolling_stats: Optional[List[str]] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    标签：由 dollar bars 生成的未来 N-bar 对数收益。\n",
    "    因子：对齐到对应区间 [start_time, end_time)（过去或未来N个bar）上基于逐笔成交直接计算。\n",
    "    返回：X_interval, y, bars\n",
    "    \"\"\"\n",
    "    bars = build_dollar_bars(trades, dollar_threshold=dollar_threshold)\n",
    "    if bars.empty:\n",
    "        return pd.DataFrame(), pd.Series(dtype=float), bars\n",
    "\n",
    "    bars = bars.reset_index(drop=True)\n",
    "    bars['bar_id'] = bars.index\n",
    "    close_s = bars.set_index('bar_id')['close']\n",
    "\n",
    "    # 未来 N-bar 对数收益\n",
    "    y = np.log(close_s.shift(-horizon_bars) / close_s)\n",
    "\n",
    "    # 计算每个样本的区间\n",
    "    features = []\n",
    "    for bar_id in close_s.index:\n",
    "        if window_mode == 'past':\n",
    "            start_idx = bar_id - feature_window_bars\n",
    "            end_idx = bar_id - 1\n",
    "            if start_idx < 0:\n",
    "                features.append({'bar_id': bar_id, '_skip': True})\n",
    "                continue\n",
    "        else:  # future 区间（注意信息泄露，仅在需要时使用）\n",
    "            start_idx = bar_id\n",
    "            end_idx = bar_id + feature_window_bars - 1\n",
    "            if end_idx >= len(bars):\n",
    "                features.append({'bar_id': bar_id, '_skip': True})\n",
    "                continue\n",
    "\n",
    "        start_ts = bars.loc[start_idx, 'start_time']\n",
    "        end_ts = bars.loc[end_idx, 'end_time']\n",
    "        feat = _compute_interval_trade_features(trades, start_ts, end_ts)\n",
    "        feat['bar_id'] = bar_id\n",
    "        feat['interval_start'] = start_ts\n",
    "        feat['interval_end'] = end_ts\n",
    "        features.append(feat)\n",
    "\n",
    "    X = pd.DataFrame(features).set_index('bar_id')\n",
    "    if '_skip' in X.columns:\n",
    "        keep_idx = X['_skip'] != True\n",
    "        X = X.loc[keep_idx].drop(columns=['_skip'])\n",
    "    \n",
    "    # 对齐标签\n",
    "    y = y.loc[X.index]\n",
    "\n",
    "    # 可选：对区间因子再做滞后/滚动（通常不需要，默认不加）\n",
    "    if add_lags or rolling_windows:\n",
    "        X = _add_bar_lags_and_rollings(\n",
    "            X,\n",
    "            add_lags=add_lags,\n",
    "            rolling_windows=rolling_windows,\n",
    "            rolling_stats=rolling_stats,\n",
    "        ).dropna()\n",
    "        y = y.loc[X.index]\n",
    "\n",
    "    return X, y, bars\n",
    "\n",
    "\n",
    "def run_bar_interval_pipeline(\n",
    "    trades: pd.DataFrame,\n",
    "    dollar_threshold: float,\n",
    "    feature_window_bars: int = 10,\n",
    "    horizon_bars: int = 3,\n",
    "    window_mode: str = 'past',\n",
    "    n_splits: int = 5,\n",
    "    embargo_bars: Optional[int] = None,\n",
    "    model_type: str = 'ridge',\n",
    "    random_state: int = 42,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    按 N 个 dollar bar 定义的时间区间计算因子，标签为未来 N-bar 对数收益；\n",
    "    使用区间的真实 end_time 作为索引做 Purged K-Fold，embargo 由 bar 中位时长换算。\n",
    "    \"\"\"\n",
    "    X, y, bars = make_interval_feature_dataset(\n",
    "        trades=trades,\n",
    "        dollar_threshold=dollar_threshold,\n",
    "        feature_window_bars=feature_window_bars,\n",
    "        horizon_bars=horizon_bars,\n",
    "        window_mode=window_mode,\n",
    "    )\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        return {'error': '数据不足或阈值设置过大，无法构造区间数据集', 'X': X, 'y': y, 'bars': bars}\n",
    "\n",
    "    # 用对应样本的区间结束时间作为索引\n",
    "    # 若为 past 模式：使用当前锚点 bar 的 end_time 代表预测时点\n",
    "    end_times = bars.set_index('bar_id')['end_time']\n",
    "    idx_time = pd.to_datetime(end_times.loc[X.index])\n",
    "    X2 = X.copy(); X2.index = idx_time\n",
    "    y2 = y.copy(); y2.index = idx_time\n",
    "\n",
    "    # embargo 转换为时间长度\n",
    "    durations = (bars['end_time'] - bars['start_time']).dropna()\n",
    "    median_duration = durations.median() if not durations.empty else pd.Timedelta(0)\n",
    "    # 自动放大 embargo：至少覆盖 feature_window_bars 的时间长度；若用户给了 embargo_bars，则取两者较大\n",
    "    auto_embargo_td = median_duration * int(max(1, feature_window_bars))\n",
    "    if embargo_bars is not None:\n",
    "        user_embargo_td = median_duration * int(max(0, embargo_bars))\n",
    "        embargo_td = max(auto_embargo_td, user_embargo_td)\n",
    "    else:\n",
    "        embargo_td = auto_embargo_td\n",
    "\n",
    "    eval_result = purged_cv_evaluate(\n",
    "        X=X2,\n",
    "        y=y2,\n",
    "        n_splits=n_splits,\n",
    "        embargo=embargo_td,\n",
    "        model_type=model_type,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return {'eval': eval_result, 'X': X, 'y': y, 'bars': bars}\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'purged_cv_evaluate',\n",
    "    'make_barlevel_dataset',\n",
    "    'run_barlevel_pipeline',\n",
    "    'make_interval_feature_dataset',\n",
    "    'run_bar_interval_pipeline',\n",
    "]\n",
    "\n",
    "def generate_date_range(start_date, end_date):    \n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    date_list = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        date_list.append(current.strftime('%Y-%m-%d'))\n",
    "        current += timedelta(days=1)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = []\n",
    "date_list = generate_date_range('2025-01-01', '2025-01-02')\n",
    "for date in date_list:\n",
    "    raw_df.append(pd.read_csv(f'/Volumes/Ext-Disk/data/futures/um/daily/trades/ETHUSDT/ETHUSDT-trades-{date}.zip'))\n",
    "    \n",
    "# dollar_bar = build_dollar_bars(raw_df, 10000 * 2000)\n",
    "# print(dollar_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df = pd.concat(raw_df, ignore_index=True)\n",
    "res = run_bar_interval_pipeline(\n",
    "    trades=trades_df,\n",
    "    dollar_threshold=10000 * 2000,\n",
    "    feature_window_bars=10,\n",
    "    horizon_bars=3,\n",
    "    window_mode='past',\n",
    "    n_splits=5,\n",
    "    embargo_bars=None,\n",
    "    model_type='ridge',\n",
    ")\n",
    "print(res.get('eval', {}).get('summary'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
